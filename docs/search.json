[
  {
    "objectID": "posts/2022-03-08-nw-reno-roads/index.html",
    "href": "posts/2022-03-08-nw-reno-roads/index.html",
    "title": "Northwest Reno streetmap - dataviz fun on the side",
    "section": "",
    "text": "In this project, I create a little map of northwest Reno, NV, streets using the osmdata (OpenStreetMap) package.\nCredit: Thanks to Joshua McCrain for this great post!"
  },
  {
    "objectID": "posts/2022-03-08-nw-reno-roads/index.html#packages-used",
    "href": "posts/2022-03-08-nw-reno-roads/index.html#packages-used",
    "title": "Northwest Reno streetmap - dataviz fun on the side",
    "section": "Packages used:",
    "text": "Packages used:\n\ntidyverse: Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\nshowtext: Yixuan Qiu and authors/contributors of the included software. See file AUTHORS for details. (2022). showtext: Using Fonts More Easily in R Graphs. R package version 0.9-5. https://CRAN.R-project.org/package=showtext\nosmdata: Mark Padgham, Bob Rudis, Robin Lovelace, Maëlle Salmon (2017). osmdata Journal of Open Source Software, 2(14). URL https://doi.org/10.21105/joss.00305\nggmap: D. Kahle and H. Wickham. ggmap: Spatial Visualization with ggplot2. The R Journal, 5(1), 144-161. URL http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf"
  },
  {
    "objectID": "posts/2022-03-08-nw-reno-roads/index.html#make-a-lil-street-map",
    "href": "posts/2022-03-08-nw-reno-roads/index.html#make-a-lil-street-map",
    "title": "Northwest Reno streetmap - dataviz fun on the side",
    "section": "Make a lil’ street map!",
    "text": "Make a lil’ street map!\n\n# Attach packages:\nlibrary(tidyverse)\nlibrary(osmdata)\nlibrary(showtext)\nlibrary(ggmap)\nlibrary(ggforce)\nlibrary(here)\n\n# Import font\nfont_add_google(name = \"Gilda Display\", family = \"gilda\") # add custom fonts\nshowtext_auto()\n\n# Get bounding box for Reno\nreno <- getbb(\"Reno Nevada\")\n\n# Get main roads, medium roads, small roads, and rivers\nreno_main <- reno %>%\n  opq()%>%\n  add_osm_feature(key = \"highway\", \n                  value = c(\"motorway\", \"primary\", \"motorway_link\", \"primary_link\")) %>%\n  osmdata_sf()\n\nreno_med <- reno %>%\n  opq()%>%\n  add_osm_feature(key = \"highway\", \n                  value = c(\"secondary\", \"tertiary\", \"secondary_link\", \"tertiary_link\")) %>%\n  osmdata_sf()\n\nreno_small <- reno %>%\n  opq()%>%\n  add_osm_feature(key = \"highway\", \n                  value = c(\"residential\", \"living_street\")) %>%\n  osmdata_sf()\n\n# Access Truckee River\nreno_river <- reno %>%\n  opq()%>%\n  add_osm_feature(key = \"waterway\", value = \"river\") %>%\n  osmdata_sf()\n\n# Create endpoints of the arced arrow (to annotate the Truckee River)\narrow <- data.frame(x1 = -119.91, x2 = -119.925, y1 = 39.485, y2 = 39.51)\n\narrow_2 <- data.frame(x1 = -119.99, x2 = -120.01, y1 = 39.44, y2 = 39.435)\n\n# Combine them in a map! \nggplot() +\n  geom_sf(data = reno_main$osm_lines, size = 0.1) +\n  geom_sf(data = reno_med$osm_lines, size = 0.1) +\n  geom_sf(data = reno_small$osm_lines, size = 0.1) +\n  geom_sf(data = reno_river$osm_lines, size = 0.5, color = \"cyan4\") +\n  geom_curve(data = arrow, aes(x = x1, y = y1, xend = x2, yend = y2),\n             arrow = arrow(length = unit(0.02, \"npc\")),\n             color = \"cyan4\") +\n  geom_curve(data = arrow_2, aes(x = x1, y = y1, xend = x2, yend = y2),\n             arrow = arrow(length = unit(0.02, \"npc\")),\n             color = \"gray70\",\n             curvature = -0.2) +\n  coord_sf(xlim = c(-120.0, -119.75), ylim = c(39.41, 39.55)) +\n  theme_void() +\n  theme(text = element_text(family = \"gilda\")) +\n  annotate(geom = \"text\", \n           x = -119.915, y = 39.46, \n           label = \"Northwest Reno, NV\",\n           family = \"gilda\",\n           size = 30) +\n  annotate(geom = \"text\",\n           x = -119.92, y = 39.48,\n           label = \"Truckee River\",\n           family = \"gilda\",\n           color = \"cyan4\",\n           size = 20\n           ) +\n  annotate(geom = \"text\",\n           x = -119.965, y = 39.443,\n           label = \"to Truckee, CA\",\n           family = \"gilda\",\n           color = \"gray60\",\n           size = 20\n           ) +\n  geom_circle(aes(x0 = -119.895, y0 = 39.515, r = 0.004), fill = \"white\", color = \"gray20\", size = 0.3) +\n  annotate(geom = \"text\",\n           x = -119.895, y = 39.515,\n           label = \"80\",\n           family = \"gilda\",\n           color = \"gray20\",\n           size = 12\n           ) +\n  geom_circle(aes(x0 = -119.782, y0 = 39.50, r = 0.005), fill = \"white\", color = \"gray20\", size = 0.3) +\n  annotate(geom = \"text\",\n           x = -119.782, y = 39.50,\n           label = \"580\",\n           family = \"gilda\",\n           color = \"gray20\",\n           size = 12\n           )\n\nggsave(\"featured_nw_reno.png\", height = 4, width = 7, dpi = 600)\n\n\nknitr::include_graphics(here::here(\"posts\", \"2022-03-08-nw-reno-roads\", \"featured_nw_reno.png\"))"
  },
  {
    "objectID": "posts/2019-05-30-horst-lab-openscapes-case-study/index.html",
    "href": "posts/2019-05-30-horst-lab-openscapes-case-study/index.html",
    "title": "Openscapes Champions Case Study: Horst Lab",
    "section": "",
    "text": "Note: I, with my teaching assistant extraordinaire Jessica Couture, participated in the 10-week Openscapes Mentorship Program led by Dr. Julia Lowndes in 2019. Here’s a wrap-up we wrote of some of our efforts and progress through Openscapes!"
  },
  {
    "objectID": "posts/2019-05-30-horst-lab-openscapes-case-study/index.html#crossposted-at-openscapes.org",
    "href": "posts/2019-05-30-horst-lab-openscapes-case-study/index.html#crossposted-at-openscapes.org",
    "title": "Openscapes Champions Case Study: Horst Lab",
    "section": "Crossposted at openscapes.org",
    "text": "Crossposted at openscapes.org\nWe have just concluded our inaugural cohort of Openscapes Champions. While sad to conclude, all Champion labs have so many exciting accomplishments and so much momentum for open data science, and it is truly just the beginning. Here we are posting individual case studies of accomplishments from Champions labs.\n\nThe Horst Lab teaches stats & data analysis to environmental science grad students at the Bren School for Environmental Science and Management at the University of California at Santa Barbara. Participating in Openscapes is lecturer Dr. Allison Horst and PhD candidate and lead teaching assistant Jessica Couture.\n\nAllison: My favorite part of teaching is seeing initially (data) intimidated students grow throughout the quarter(s) to become confident and curious R-users who can responsibly wrangle and analyze their own real-world data, then successfully communicate results.\nJessica: I study marine aquaculture and sustainable food systems and incorporate open science practices in my work. I love teaching data and open science methods with Allison because it is great to be able to lower the barriers to these practices and move more scientists towards open and collaborative work.\n\n\n\n\n\nTidyverse nebula by Allison Horst\n\n\n\nThe Horst Lab Case Study shares our accomplishments for streamlining how we teach two courses. The introductory course, ESM 206, is ~100 graduate students from Bren and other departments, and additionally the optional advanced course, ESM 244, reached enrollment of 68 students from departments across campus in 2019! In both courses, we emphasize open practices and tools (R, RStudio, GitHub) for reproducibility and collaboration, along with data analysis concepts and methods. As a preview, we learned that:\n\n“being a champion for open data science doesn’t always mean being an expert in R or GitHub (or data science more generally). It means that I can be a champion by helping other people (like my students!) realize the value of open, reproducible, collaborative data analysis and give them the confidence and curiosity to learn useful tools so they can do it on their own.”\n\nOur key accomplishments are:\n\nConverted ESM 206 / 244 lectures to Google Slides for sharing\nCollected all ESM 206 / 244 lab materials, now each an .Rproj, shared on GitHub!\nAdded GitHub to course goals for MESM students\nEstablished weekly #tidytuesday hacky hours with Gracie White\nEngaged w/ online #rstats community, learned a TON (esp. about useful functions, dataviz, and how awesome people are in the R community)\nThrough twitter, connected with a lot of people in the R community\nLearned to deal with more Git merge conflicts in RStudio and in the command line (excited to teach this in ESM 206 next year!)\nMerged first pull request\nSo many great efficiency/keyboard shortcut tips! I feel so cool when I use them!\nAlmost done making our teaching website, PSYCHED\nLearned a little about css & html (by fumbling)\nCreated a course code of conduct to promote positivity and inclusivity in ESM 206 / 244\n\n\n\nCongratulations Allison, Jessica, and all future TAs and students that are lucky enough to take your classes!\n\n\nRelevant posts:\n\nPersonifying code\nOpenscapes Champions incorporate open practices in their science\nOpenscapes summit reflections — becoming champions\nOpenscapes summit reflections 2 — Changing the way we do science\nStarting our #tidytuesday hacky hours"
  },
  {
    "objectID": "posts/2020-03-01-r-for-excel-users-at-rstudioconf-2020/index.html",
    "href": "posts/2020-03-01-r-for-excel-users-at-rstudioconf-2020/index.html",
    "title": "R for Excel Users Workshop",
    "section": "",
    "text": "tl;dr: all workshop materials are available here:  GitHub: https://github.com/rstudio-conf-2020/r-for-excel  Book: https://rstudio-conf-2020.github.io/r-for-excel/  Slides  Cross posted: https://education.rstudio.com/blog and https://www.openscapes.org/blog/  License: CC BY-SA 4.0"
  },
  {
    "objectID": "posts/2020-03-01-r-for-excel-users-at-rstudioconf-2020/index.html#background",
    "href": "posts/2020-03-01-r-for-excel-users-at-rstudioconf-2020/index.html#background",
    "title": "R for Excel Users Workshop",
    "section": "Background",
    "text": "Background\nWe were thrilled to co-teach the R for Excel Users workshop at rstudio::conf(2020)! From early on in our weekly early-morning work sessions and brainstorming hikes, we knew that our R for Excel Users workshop would not be about wholesale translating Excel operations into R. Instead, it would be a more holistic approach to reproducible analyses with R – a friendly introduction to becoming a modern R user.\n\n\n\nArt by Allison Horst\n\n\nOur philosophy was always about empowering our attendees to continue learning R after our workshop – and so that they would be excited about it. We wanted to teach not only the skillsets of the modern R user, but also the habits and mindsets for working in a reproducible and collaborative way. Through hands-on experience our attendees would build skills and confidence at the workshop, and through stories and packages from the #rstats community, they would build the mindset to expect what they want to do is possible and have a good start to finding it.\nThus, attendees would learn:\n\ncoding with best practices (RStudio/tidyverse)\ncollaborative bookkeeping (Git/GitHub)\nreporting and publishing (RMarkdown/GitHub)\n\nAnd they would build the mindset:\n\nto expect that what they want to do is possible\nto have confidence they can find it\nto continue learning with supportive community\nto enable others\n\nFocusing on #rstats communities at our workshop was especially important to us because of our audience. These folks were coming to a 2400-person conference all about R – and they did not yet know R! That seems scary! Anything we could do to not only teach them about reproducible workflows but also make the conference easier to navigate for a newcomer we felt was time well spent. Thus, we deliberately focused time on R learning communities that they would see and can get involved in at the conference and afterwards. And, we included a discussion about how Twitter is a legit tool for R, and how to get started."
  },
  {
    "objectID": "posts/2020-03-01-r-for-excel-users-at-rstudioconf-2020/index.html#teaching-at-rstudioconf",
    "href": "posts/2020-03-01-r-for-excel-users-at-rstudioconf-2020/index.html#teaching-at-rstudioconf",
    "title": "R for Excel Users Workshop",
    "section": "Teaching at RStudio::conf!",
    "text": "Teaching at RStudio::conf!\nWe taught R for Excel Users at RStudio::conf(2020) with our amazing TAs (and fellow Santa Barbara R-Ladies) Jessica Couture and Jamie Montgomery! We jumped into Day 1 of the workshop with 25 optimistic participants from industry, government offices, non-profits, and even several RStudio employees.\n\n\n\nJulie, Jessica, Allison, and Jamie at RStudio::conf(2020)!\n\n\nOur workshop assumed no previous experience with R or RStudio. After setting the tone and expectations of the workshop with how R is like the Force from Star Wars, we spent the first two sessions on meeting, exploring, and setting up our tools, including:\n\nMeeting the RStudio IDE, Hello R Markdown!, Packages, Functions\nWorking between GitHub and R projects\n\nWith Jessica and Jamie’s awesome troubleshooting, we were all off and running at the end of Session 2 with RStudio, R Markdown, and GitHub and installed, running, and somewhat familiar. Of course after getting set up with our software, we had work to do! We spent the rest of the day:\n\nMaking ggplot graphs\npivot tables in R with group_by() %>% summarize()\n\nOur fearless attendees returned on Day 2, with a chorus of “Alligator mouth dash” (<-) in response to Alison’s question “How do we assign an object in R?”. We immediately jumped into coding by:\n\nReshaping, separating and uniting things with tidyr\nFiltering and joining data\nGetting help, #rstats communities, and collaborating/fixing merge conflicts in GitHub\nSynthesis of the skills and tools we’d learned\n\nOur final session was revisiting and practicing all that we had learned throughout the workshop while participants also collaborated on RMarkdown files with each other through GitHub.\nThe vibe throughout the workshop was fun and lighthearted, which is exactly what we were hoping for. In addition to “Alligator mouth dash”, my favorite moment in the whole workshop was an audible whisper (“very nice!”) from someone using separate() for the first time. A close second was the cheers for janitor::clean_names() and Allison’s responding, “Yes, this is worth cheering over!”. Our attendees were hard-working folks with a great attitude and sense of humor, who are now equipped for more exciting steps in their R journey."
  },
  {
    "objectID": "posts/2020-03-01-r-for-excel-users-at-rstudioconf-2020/index.html#whats-next",
    "href": "posts/2020-03-01-r-for-excel-users-at-rstudioconf-2020/index.html#whats-next",
    "title": "R for Excel Users Workshop",
    "section": "What’s next",
    "text": "What’s next\nWe loved developing and teaching R for Excel Users, and hope to do it again. Our course materials evolved quite a bit throughout months of development, and were hugely helped by a pilot workshop we ran at UC Santa Barbara in early December. Even so, there are little things we would like to iron out and improve upon for next time. And we will also update the Book with images from the Slides so that the Book is completely self-contained and the Slides are a bonus, rather than a necessity.\nWe also got feedback that we should change the name of the course because while it has some framing from Excel, it is certainly not limited to Excel Users. This seems like a good idea. When we were first talking to RStudio’s Alison Hill about the workshop, we discussed how it could be somewhat of a precursor to Jenny Bryan and Jim Hester’s What They Forgot to Teach You About R – ours being What You Should Know When You Learn R. Jenny and Jim’s have a great acronym, WTF, so I have been thinking about names. I learned from Jenny that WTF was a welcome byproduct that came after they came up with the workshop name, but I have to approach it as a backronym. I’ve had many terrible ideas, but! Through cocktail chats at the conference with Charlotte Wickham and Hannah Frick, we landed on TLDR being best. I love Charlotte’s suggestion of Take a Leap and Dive into R or my less-great Time to Learn Data analysis in R. This is an ongoing pursuit, and suggestions are very welcomed :).\n\n\nphoto by Brunel Johnson with art by Allison Horst"
  },
  {
    "objectID": "posts/2019-04-28-do-your-worst/index.html",
    "href": "posts/2019-04-28-do-your-worst/index.html",
    "title": "There’s value in trying your [dataviz] worst",
    "section": "",
    "text": "In the past I’ve always asked students to create their best possible graphs in ggplot2 to practice creating clear, engaging data visualizations. Recently, I’ve realized value in adding a few early exercises that encourage students to make their worst.\nWhy is it good to make ggplot2 graphs so (intentionally) bad?\nHere are four ways that creating a purposefully disgusting graph promotes learning and exploration in ggplot2:\nI was recently inspired to try my absolute dataviz worst by a #tidytuesday prompt using data from Sarah Leo’s efforts to improve imperfect graphs in The Economist.\nInstead of trying to recreate or further improve on their graphs, I decided to use my powers and energy for a much different purpose. A darker and more anarchistic purpose…\nBehold, my worst!\nThe undiscerning eye might think “these graphs are abominations and there was clearly no effort put into them.”\nWrong.\nIt’s a long dive from ggplot2 defaults into the depths of truly terrible DataViz, and it takes creativity and effort to keep sinking.\nHere are just a few things I learned or re-learned during my nosedive:\n(click here for my complete worst dataviz code)\nThrough the process of trying my worst I learned some new customization skills, created useful reference code for future ggplot2 efforts, and had fun – it felt oddly liberating to be deliberately and creatively awful.\nFrom now on, I’ll be adding a few early ggplot2 activities requiring students to do their absolute dataviz worst. There’s plenty of room at the bottom, and we can learn a lot and have fun on our way down."
  },
  {
    "objectID": "posts/2019-04-28-do-your-worst/index.html#data-visualization-resources-in-the-other-direction",
    "href": "posts/2019-04-28-do-your-worst/index.html#data-visualization-resources-in-the-other-direction",
    "title": "There’s value in trying your [dataviz] worst",
    "section": "Data visualization resources (in the other direction):",
    "text": "Data visualization resources (in the other direction):\n\nFundamentals of Data Visualization by Claus Wilke\nData Visualization: a Practical Introduction by Kieran Healy"
  },
  {
    "objectID": "posts/2022-03-10-tree-mortality-fires/index.html",
    "href": "posts/2022-03-10-tree-mortality-fires/index.html",
    "title": "Exploring tree outcomes following fires",
    "section": "",
    "text": "Basically, there’s this awesome dataset on tree survival following fires, the Fire and Tree Mortality Database, and I want to go exploring & compare fire survival across species. Some fun with tidymodels, data visualization, binary logistic regression, and my first shot at using the fantastic geomtextpath package!"
  },
  {
    "objectID": "posts/2022-03-10-tree-mortality-fires/index.html#citations",
    "href": "posts/2022-03-10-tree-mortality-fires/index.html#citations",
    "title": "Exploring tree outcomes following fires",
    "section": "Citations:",
    "text": "Citations:\nCansler et al. (2020). Fire and tree mortality database (FTM). Fort Collins, CO: Forest Service Research Data Archive. Updated 24 July 2020. https://doi.org/10.2737/RDS-2020-0001\nCansler et al. (2020). The Fire and Tree Mortality Database, for empirical modeling of individual tree mortality after fire. Scientific Data 7: 194. https://doi.org/10.1038/s41597-020-0522-7"
  },
  {
    "objectID": "posts/2022-03-10-tree-mortality-fires/index.html#attach-packages-read-in-the-data",
    "href": "posts/2022-03-10-tree-mortality-fires/index.html#attach-packages-read-in-the-data",
    "title": "Exploring tree outcomes following fires",
    "section": "Attach packages & read in the data",
    "text": "Attach packages & read in the data\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(naniar)\nlibrary(tidymodels)\nlibrary(geomtextpath)\nlibrary(paletteer)\n\n\ntrees <- read_csv(here(\"posts\", \"2022-03-10-tree-mortality-fires\", \"data\", \"Data\", \"FTM_trees.csv\")) # Tree outcomes and records\n\nImportant information: See attributes in _metadata_RDS-2020-0001.html for variable definitions."
  },
  {
    "objectID": "posts/2022-03-10-tree-mortality-fires/index.html#exploratory-data-visualization",
    "href": "posts/2022-03-10-tree-mortality-fires/index.html#exploratory-data-visualization",
    "title": "Exploring tree outcomes following fires",
    "section": "Exploratory data visualization",
    "text": "Exploratory data visualization\nCounts of tree species in the dataset:\n\n# Find the top 20 most counted tree species\ntrees <- trees %>% \n  mutate(sci_name = paste(Genus, Species_name)) %>% \n  filter(sci_name != \"Pinus jeffreyi or ponderosa\")\n\ntree_count_top_20 <- trees %>% \n  count(sci_name) %>% \n  mutate(sci_name = fct_reorder(sci_name, n)) %>% \n  slice_max(n, n = 20)\n\ntree_20_gg <- ggplot(data = tree_count_top_20, aes(x = sci_name, y = n)) +\n  geom_col() +\n  coord_flip() +\n  theme_minimal() +\n  labs(y = \"\\nObservations in dataset\",\n       x = \"Scientific name\")\n\n\n\n\n\n\n\n\n\nCounts of live (0) and dead (1) for the top 20 most recorded trees in the dataset:\n\n# Make a long form of the trees dataset (top 20 most observed tree species)\ntrees_long <- trees %>% \n  pivot_longer(cols = yr0status:yr10status, names_to = \"yr_outcome\", values_to = \"live_dead\") %>% \n  mutate(yr_since_fire = as.numeric(parse_number(yr_outcome)),\n         live_dead_chr = case_when(\n           live_dead == 0 ~ \"live\",\n           live_dead == 1 ~ \"dead\"\n         )) %>% \n  filter(sci_name %in% tree_count_top_20$sci_name)\n\ntrees_live_dead <- trees_long %>% \n  count(sci_name, yr_since_fire, live_dead_chr) %>% \n  drop_na()\n  \ntree_survival_gg <- ggplot(data = trees_live_dead, aes(x = yr_since_fire, y = n)) +\n  geom_col(aes(fill = live_dead_chr), position = \"fill\") +\n  scale_fill_manual(values = c(\"lightsalmon\", \"forestgreen\"),\n                    name = \"Live / dead:\") +\n  scale_x_continuous(breaks = c(0, 5, 10), labels = c(\"0\", \"5\", \"10\")) +\n  theme_minimal() +\n  labs(x = \"Years since fire\",\n       y = \"Proportion live / dead\",\n       title = \"Tree survival post-fire\",\n       subtitle = \"Only includes the 20 most observed trees in the dataset\",\n       caption = \"Data: Fire and tree mortality database (FTM)\") +\n  facet_wrap(~sci_name)\n\n\n\n\n\n\n\n\n\nWe can already see some interesting differences in survival across species. For example, Picea mariana and Abies lasiocarpa experience quick mortality within the first year; others like Pinus jeffreyi and Abies concolor appear more resilient. However, near-complete mortality is observed across all species within 10 years."
  },
  {
    "objectID": "posts/2022-03-10-tree-mortality-fires/index.html#ponderosa-pines---diving-a-bit-deeper",
    "href": "posts/2022-03-10-tree-mortality-fires/index.html#ponderosa-pines---diving-a-bit-deeper",
    "title": "Exploring tree outcomes following fires",
    "section": "Ponderosa pines - diving a bit deeper",
    "text": "Ponderosa pines - diving a bit deeper\nSince it is the most observed species in the dataset and because it happens to be one of my favorite trees, I’ll dive a bit deeper into factors that may influence Pinus ponderosa mortality post-fire.\n\nponderosa <- trees_long %>% \n  filter(sci_name == \"Pinus ponderosa\")\n\n# write_csv(ponderosa, here::here(\"content\", \"post\", \"2022-03-10-tree-mortality-fires\", \"ponderosa.csv\"))\n\nFirst, let’s take a look at mortality over time (years since fire):\n\nsurvival_gg <- ggplot(data = ponderosa, aes(x = yr_since_fire, y = live_dead)) +\n  geom_jitter(alpha = 0.008) +\n  labs(x = \"Years since fire\",\n       y = \"Tree status (live / dead)\",\n       title = \"Ponderosa pine mortality post-fire\",\n       caption = \"Data: Fire and tree mortality database (FTM)\") +\n  scale_y_continuous(breaks = c(0, 1), labels = c(\"Live\", \"Dead\")) +\n  scale_x_continuous(breaks = c(0, 5, 10), labels = c(\"0\", \"5\", \"10\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nClassification: binary logistic regression in tidymodels\nCreate the training & testing sets\n\nponderosa <- ponderosa %>% \n  drop_na(yr_since_fire, live_dead) %>% \n  mutate(live_dead = as.factor(live_dead))\n\n# Make the training & testing dataset:\nponderosa_split <- ponderosa %>% \n  initial_split(prop = 4/5)\n\n# Confirm the splits (Analysis/Assess/Total): \nponderosa_split\n\n<Training/Testing/Total>\n<293297/73325/366622>\n\n# Extract the training and testing sets: \nponderosa_train <- training(ponderosa_split)\nponderosa_test <- testing(ponderosa_split)\n\n# Check them out a bit: \nponderosa_train %>% \n  count(yr_since_fire, live_dead)\n\n# A tibble: 22 × 3\n   yr_since_fire live_dead     n\n           <dbl> <fct>     <int>\n 1             0 0         39615\n 2             0 1           546\n 3             1 0         39123\n 4             1 1         10420\n 5             2 0         26942\n 6             2 1         12874\n 7             3 0         23600\n 8             3 1         14918\n 9             4 0          8633\n10             4 1         14982\n# … with 12 more rows\n\nponderosa_test %>% \n  count(yr_since_fire, live_dead)\n\n# A tibble: 22 × 3\n   yr_since_fire live_dead     n\n           <dbl> <fct>     <int>\n 1             0 0         10054\n 2             0 1           143\n 3             1 0          9756\n 4             1 1          2549\n 5             2 0          6766\n 6             2 1          3220\n 7             3 0          5953\n 8             3 1          3583\n 9             4 0          2170\n10             4 1          3771\n# … with 12 more rows\n\n\n\n\nMake a recipe\n\n# Just using the single predictor here:\nponderosa_recipe <- recipe(live_dead ~ yr_since_fire, data = ponderosa)\nponderosa_recipe \n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\n\n\n\nMake the model\n\nponderosa_model <- \n  logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\") # Binary classificiation\n\n\n\nMake the workflow\n\nponderosa_wf <- workflow() %>% \n  add_recipe(ponderosa_recipe) %>% \n  add_model(ponderosa_model)\n\n\n\nFit the model:\n\nponderosa_fit <- ponderosa_wf %>% \n  last_fit(ponderosa_split)\n\n# Which returns high accuracy and roc_auc:\nponderosa_fit %>% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.807 Preprocessor1_Model1\n2 roc_auc  binary         0.878 Preprocessor1_Model1\n\n\n\n\nProof of concept: check out the test set predictions\n…just for the first 20 rows:\n\nponderosa_fit %>% \n  collect_predictions() %>% \n  head(20)\n\n# A tibble: 20 × 7\n   id               .pred_0 .pred_1  .row .pred_class live_dead .config         \n   <chr>              <dbl>   <dbl> <int> <fct>       <fct>     <chr>           \n 1 train/test split  0.393   0.607      4 1           1         Preprocessor1_M…\n 2 train/test split  0.0390  0.961      8 1           1         Preprocessor1_M…\n 3 train/test split  0.0750  0.925     21 1           1         Preprocessor1_M…\n 4 train/test split  0.565   0.435     28 0           0         Preprocessor1_M…\n 5 train/test split  0.722   0.278     30 0           1         Preprocessor1_M…\n 6 train/test split  0.565   0.435     31 0           1         Preprocessor1_M…\n 7 train/test split  0.393   0.607     32 1           1         Preprocessor1_M…\n 8 train/test split  0.140   0.860     34 1           1         Preprocessor1_M…\n 9 train/test split  0.0750  0.925     35 1           1         Preprocessor1_M…\n10 train/test split  0.0100  0.990     38 1           1         Preprocessor1_M…\n11 train/test split  0.565   0.435     42 0           0         Preprocessor1_M…\n12 train/test split  0.838   0.162     44 0           0         Preprocessor1_M…\n13 train/test split  0.722   0.278     52 0           1         Preprocessor1_M…\n14 train/test split  0.565   0.435     53 0           1         Preprocessor1_M…\n15 train/test split  0.0750  0.925     72 1           1         Preprocessor1_M…\n16 train/test split  0.0100  0.990     75 1           1         Preprocessor1_M…\n17 train/test split  0.912   0.0880    80 0           0         Preprocessor1_M…\n18 train/test split  0.838   0.162     81 0           0         Preprocessor1_M…\n19 train/test split  0.722   0.278     82 0           0         Preprocessor1_M…\n20 train/test split  0.245   0.755     88 1           1         Preprocessor1_M…\n\n\n\n\nConfusion matrix of truth / predictions\nRecall here: 0 = “Live”, 1 = “Dead”\n\nponderosa_fit %>% \n  collect_predictions() %>% \n  conf_mat(truth = live_dead, estimate = .pred_class)\n\n          Truth\nPrediction     0     1\n         0 32529  9495\n         1  4671 26630\n\n\n\n\nFit on entire dataset\n\nponderosa_model_full <- fit(ponderosa_wf, ponderosa)\n\nponderosa_model_full\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n  (Intercept)  yr_since_fire  \n      -2.3424         0.6929  \n\nDegrees of Freedom: 366621 Total (i.e. Null);  366620 Residual\nNull Deviance:      508200 \nResidual Deviance: 318000   AIC: 318000\n\n\n\n\nMaking new predictions\nLet’s say we want to predict survival of other ponderosa pines based solely on years post-fire:\n\n# Make a data frame containing a \"yr_since_fire\" variable as a new model input:\nnew_yr <- data.frame(yr_since_fire = c(0, 0.4, 1, 2.2, 5.7, 8.3))\n\n# Then use the model to predict outcomes, bind together: \nexample_predictions <- data.frame(new_yr, predict(ponderosa_model_full, new_yr))\n\nexample_predictions\n\n  yr_since_fire .pred_class\n1           0.0           0\n2           0.4           0\n3           1.0           0\n4           2.2           0\n5           5.7           1\n6           8.3           1\n\n\nThis does seem to align with what we’d expect based on the data visualization. We can also find the probability of “Dead” (outcome = 1) using the model predictions, adding type = \"prob\" within the predict() function.\n\npredict_over <- data.frame(yr_since_fire = seq(from = 0, to = 10, by = 0.1))\npredictions_full <- data.frame(predict_over, predict(ponderosa_model_full, predict_over, type = \"prob\"))\nnames(predictions_full) <- c(\"yr_since_fire\", \"prob_alive\", \"prob_dead\")\n\n# Plot probability of mortality:\nponderosa_prob_alive <- ggplot() +\n  geom_line(data = predictions_full, aes(x = yr_since_fire, y = prob_alive), color = \"gray30\", size = 1) +\n  labs(x = \"Years since fire\",\n       y = \"Probability of tree being alive\",\n       title = \"Predicted Ponderosa pine mortality post-fire\",\n       caption = \"Data: Fire and tree mortality database (FTM)\") +\n  scale_y_continuous(breaks = c(0, 0.5,  1), \n                     labels = c(\"0%\", \"50%\", \"100%\"),\n                     limits = c(0, 1)) +\n  scale_x_continuous(breaks = c(0, 5, 10), labels = c(\"0\", \"5\", \"10\")) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2022-03-10-tree-mortality-fires/index.html#extending-the-model",
    "href": "posts/2022-03-10-tree-mortality-fires/index.html#extending-the-model",
    "title": "Exploring tree outcomes following fires",
    "section": "Extending the model",
    "text": "Extending the model\nI want to extend this model for the 20 most observed trees in the dataset (so will include species as a predictor variable).\nCreate the training & testing sets\n\ntrees_20 <- trees_long %>% \n  filter(sci_name %in% c(tree_count_top_20$sci_name)) %>% \n  drop_na(yr_since_fire, live_dead) %>% \n  mutate(live_dead = as.factor(live_dead))\n\n# Make the training & testing dataset:\ntrees_20_split <- trees_20 %>% \n  initial_split(prop = 4/5)\n\n# Confirm the splits (Analysis/Assess/Total): \ntrees_20_split\n\n<Training/Testing/Total>\n<830790/207698/1038488>\n\n# Extract the training and testing sets: \ntrees_20_train <- training(trees_20_split)\ntrees_20_test <- testing(trees_20_split)\n\n# Check them out a bit: \ntrees_20_train %>% \n  count(yr_since_fire, live_dead)\n\n# A tibble: 22 × 3\n   yr_since_fire live_dead     n\n           <dbl> <fct>     <int>\n 1             0 0         77613\n 2             0 1          2907\n 3             1 0         73260\n 4             1 1         30596\n 5             2 0         51524\n 6             2 1         47193\n 7             3 0         43290\n 8             3 1         54749\n 9             4 0         22448\n10             4 1         55663\n# … with 12 more rows\n\ntrees_20_test %>% \n  count(yr_since_fire, live_dead)\n\n# A tibble: 22 × 3\n   yr_since_fire live_dead     n\n           <dbl> <fct>     <int>\n 1             0 0         19513\n 2             0 1           747\n 3             1 0         18503\n 4             1 1          7598\n 5             2 0         13012\n 6             2 1         11873\n 7             3 0         10671\n 8             3 1         13719\n 9             4 0          5578\n10             4 1         13780\n# … with 12 more rows\n\n\n\nMake a recipe\n\n# Just using the single predictor here:\ntrees_20_recipe <- recipe(live_dead ~ yr_since_fire + sci_name, data = trees_20)\ntrees_20_recipe\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          2\n\n\n\n\nMake the model\n\ntrees_20_model <- \n  logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\") # Binary classificiation\n\n\n\nMake the workflow\n\ntrees_20_wf <- workflow() %>% \n  add_recipe(trees_20_recipe) %>% \n  add_model(trees_20_model)\n\n\n\nFit the model:\n\ntrees_20_fit <- trees_20_wf %>% \n  last_fit(trees_20_split)\n\n# Which returns high accuracy and roc_auc:\ntrees_20_fit %>% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.820 Preprocessor1_Model1\n2 roc_auc  binary         0.893 Preprocessor1_Model1\n\n\n\n\nConfusion matrix of truth / predictions\nRecall here: 0 = “Live”, 1 = “Dead”\n\ntrees_20_fit %>% \n  collect_predictions() %>% \n  conf_mat(truth = live_dead, estimate = .pred_class)\n\n          Truth\nPrediction      0      1\n         0  57782  20086\n         1  17403 112427\n\n\n\n\nFit on entire dataset\n\ntrees_20_model_full <- fit(trees_20_wf, trees_20)\n\ntrees_20_model_full\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n                  (Intercept)                  yr_since_fire  \n                     -1.89320                        0.61524  \n        sci_nameAbies grandis       sci_nameAbies lasiocarpa  \n                      0.69818                        2.72269  \n          sci_nameAcer rubrum   sci_nameCalocedrus decurrens  \n                      0.54265                        0.24419  \n sci_nameJuniperus scopulorum     sci_nameLarix occidentalis  \n                      0.94475                       -1.21916  \n    sci_namePicea engelmannii          sci_namePicea mariana  \n                      1.88703                        5.68710  \n     sci_namePinus albicaulis         sci_namePinus contorta  \n                      2.17161                        1.78648  \n       sci_namePinus echinata         sci_namePinus jeffreyi  \n                      0.56136                       -0.54492  \n    sci_namePinus lambertiana        sci_namePinus palustris  \n                      0.30286                       -1.42278  \n      sci_namePinus ponderosa            sci_namePinus taeda  \n                     -0.21649                        0.06294  \n  sci_namePopulus tremuloides  sci_namePseudotsuga menziesii  \n                      1.30545                       -0.27530  \n   sci_nameTsuga heterophylla  \n                      0.82540  \n\nDegrees of Freedom: 1038487 Total (i.e. Null);  1038467 Residual\nNull Deviance:      1358000 \nResidual Deviance: 818700   AIC: 818700\n\n\n\n\nMortality (probability)\n\n# Make a data frame containing a \"sci_name\" and \"yr_since_fire\" variable as a new model input:\nnew_data <- data.frame(sci_name = rep(unique(trees_20$sci_name), 100)) %>% \n  arrange(sci_name)\n\nnew_data <- data.frame(new_data, yr_since_fire = rep(seq(from = 0, to = 10, length = 100), 20))\n\ntree_20_predictions <- data.frame(new_data, predict(trees_20_model_full, new_data, type = \"prob\"))\nnames(tree_20_predictions) <- c(\"sci_name\", \"yr_since_fire\", \"prob_alive\", \"prob_dead\")\n\n# Plot probability of mortality:\nall_prob_gg <- ggplot() +\n  geom_textline(data = tree_20_predictions, \n                aes(x = yr_since_fire, \n                    y = prob_alive, \n                    label = sci_name,\n                    group = sci_name,\n                    color = sci_name), \n                size = 2.5,\n                show.legend = FALSE) +\n  labs(x = \"Years since fire\",\n       y = \"Probability of tree being alive\",\n       title = \"Predicted tree mortality post-fire\",\n       caption = \"Data: Fire and tree mortality database (FTM)\") +\n  scale_y_continuous(breaks = c(0, 0.5,  1), \n                     labels = c(\"0%\", \"50%\", \"100%\"),\n                     limits = c(0, 1)) +\n  scale_x_continuous(breaks = c(0, 5, 10), labels = c(\"0\", \"5\", \"10\")) +\n  scale_color_paletteer_d(\"ggthemes::Tableau_20\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2022-03-10-tree-mortality-fires/index.html#more-opportunities",
    "href": "posts/2022-03-10-tree-mortality-fires/index.html#more-opportunities",
    "title": "Exploring tree outcomes following fires",
    "section": "More opportunities",
    "text": "More opportunities\nThere are a bunch of other variables in this dataset that would be worth considering - like how scorched the tree is post-fire, how large it was to start (height and diameter), evidence of beetle infestation, and more - I’m looking forward to coming back to this dataset in the future & revisiting this model with additional investigation of those variables."
  },
  {
    "objectID": "posts/2020-10-12-tidydata-openscapes/index.html",
    "href": "posts/2020-10-12-tidydata-openscapes/index.html",
    "title": "Tidy data for collaboration, efficiency and reproducibility",
    "section": "",
    "text": "Check out our blog post, including original illustrations, HERE.\nMake friends with tidy data!"
  },
  {
    "objectID": "posts/2022-03-07-scrape-dart-fish/index.html",
    "href": "posts/2022-03-07-scrape-dart-fish/index.html",
    "title": "Scraping, wrangling & viz, oh my! Fun with Columbia Basin DART (fish passage data)",
    "section": "",
    "text": "Check out the exploratory document: https://horst.shinyapps.io/dart_fish/\nVisit the project repo: https://github.com/allisonhorst/dart_salmon_passage\n\nThis project involved:\n\nA little crawl over web pages with purrr (and loving the side effects, like possibly()!)\nWeb scraping with rvest to access fish count tables from > 1500 unique URLs\nCreating a function to access an html table, read it in, and combine with other scraped tables\nData wrangling (dplyr)\nData visualization (ggplot2)\nInteractive visualizations (shiny widgets + reactive outputs)\nNext steps: time series analysis & forecasting!"
  },
  {
    "objectID": "posts/2020-05-30-teach-r-with-learnr/index.html",
    "href": "posts/2020-05-30-teach-r-with-learnr/index.html",
    "title": "Teach R with learnr",
    "section": "",
    "text": "Check out my new post for RStudio Ed, describing how I use the learnr package to facilitate teaching and learning for remote classes with R beginners!"
  },
  {
    "objectID": "posts/2021-03-31-joining-teams/index.html",
    "href": "posts/2021-03-31-joining-teams/index.html",
    "title": "Merge conflicts: helping data science students merge their advanced skills into existing teams",
    "section": "",
    "text": "Students in data science programs build advanced skills in popular programming languages (e.g. Python, R, SQL, etc), platforms (e.g. RStudio, Jupyter Notebooks) and workflows (e.g. using version control with git). Entering the workplace, however, data science program graduates will often join teams further from the leading edge of data science tools and approaches. Having a junior employee with more sophisticated technical skills than the senior members can cause complications for both the junior employee and the team as a whole. We believe preparing students for merging their advanced skills into existing teams needs to be included in data science degree programs.\n\n“Hello, world work.”\nData science programs often bake coding agility into their teaching curriculum by teaching and using different languages and platforms in different courses, for example how the UBC Master of Data Science program thoughtfully integrates R and Python. Students also learn workflows and tools for reproducible, collaborative, cloud-based data science, putting them at the forefront of modern data science skills. Those same advanced skills that get a graduate a job, however, can cause conflict if not thoughtfully integrated into existing team practices. You can imagine if a program spends several years training a student in Python, and the team is entirely using Microsoft Access there is going to be a shock for all parties involved.\nIf the goal of data science programs is to prepare students for their careers, that should also include efforts to: (1) understand - and realistically consider - the landscapes that graduates will be entering (re: tools, conventions, etc. where our grads are applying for and getting jobs), and (2) explicitly teach strategies to integrate advanced skills into teams.\nFor example, how can/should a graduate adapt from a git/GitHub workflow into a team that uses a shared folder to manage and share files? How can/should a graduate writing reproducible code in R or Python integrate into a collaborative project existing in Google Sheets or Excel? How can/should a graduate trained in cloud computing work with and benefit a team with colleagues that work entirely locally?\n\n\nWait, does that actually happen?\nYep.\nData science grads are unlikely to join data science teams working with identical data science tools, workflows and mindsets they learned in their curriculum. The difference may be a crack or a chasm - depending on both the graduate and the team they’re joining - but some dissonance should be expected.\nThe gap may be widest for data science graduates joining science teams with field specific expertise, but without formal data science training. For example, some of our Master of Environmental Science and Management graduates, who have built substantial data science skills in R, ArcGIS / QGIS, git, GitHub, bash, and more, will join environmental consulting firms, non-profits, and NGOs that are primarily working - and doing genuinely great work - in Excel or Google Sheets.\nSo, how do we train our graduates for that scenario?\n\n\n\nAllison sidebar\n\n\nHi all, I'm looking for resources to help grads from DS programs (e.g. working in R/Py/SQL) integrate into teams using Excel. Like a “Good enough practices” paper w/ strategies to join & support an Excel-based team? Thanks for suggestions! @CMastication @robinson_es @skyetetra pic.twitter.com/WPMhzWU2wc\n\n— Allison Horst (@allison_horst) March 2, 2021\n\n\nRecently I tried to crowdsource resources for this problem on twitter. Two things stood out about the responses:\n\n\n\nDespite wide reach (>48,000 impressions and 31 retweets - including by influential software developers and data science educators), there were only six responses with actual suggestions\n\n\nOf those six suggestions, five were to suggest tools and/or packages to work between languages and/or platforms (e.g. openxlsx, googlesheets4, Power BI, xlwings).\n\n\n\nI haven’t systematically reviewed data science programs or literature for how these skills are included, but based on these responses my sense is the following: Resources to help data science program graduates integrate their advanced skills into existing teams are sparse, and the focus is often on tools to interface between tools, rather than professional and interpersonal strategies to join a team._\n\n\n\nTeaching tools to nimbly move between languages and platforms is important. For example, R packages like openxlsx and googlesheets4 provide concrete paths forward for collaboration between team members split between coding in R and working in spreadsheets. However, at some level these aren’t problems with tools so much as problems with conflicting human interests. Even if a programming language package effectively interfaces with a different tool, you still have to convince a team of people to use that interface. We believe there are two common scenarios that unfold due to the human nature of this problem, which we’ll describe next.\n\n\nOK…well what are students doing now without that training?\nThere are two common scenarios that often occur when a student is trained in skills that are more advanced than required for their first job out of school. In one scenario the student doesn’t use the skills they were trained in, and adapts to the role. In the other scenario the student tries to get the change organization to use the skills the student was trained. For the canonical example of a student trained in R/Python/SQL joining a team that exclusively uses Excel, either the student will switch to purely using Excel themselves or try and teach the organization to use some R, Python, or SQL.\nIn the scenario where the recent graduate chooses to adapt to the existing technology of the team, this harms both the recent graduate and the team itself. The graduate is harmed because without the ability to practice the skills they learned in school those skills will likely atrophy. For example, git is a fairly complex skill to learn with a lot of nuanced components (like the difference between committing and staging). If a former student doesn’t continuously use those skills, it will be much harder to relearn them later. Further, jobs that require more advanced skills like git are often desirable to data scientists, and having years of only using Excel will make it harder to interview for and obtain such a job in the future.\nMeanwhile, if a recent data science graduate backslides entirely into tools that a company is using that company is missing out on a valuable vector to advance their data science tools and workflows. It’s very common for teams to get stuck using outdated technology because that’s what people on the team are used to, and at a certain point a critical mass is reached where because so many people on the team are unwilling (or unable) to change, it becomes impossible to do so. New graduates with data science skills are a great way to avoid data science stagnancy. They can introduce new ideas and skills into an organization since they have been recently trained on the skills and have a fresh perspective. As an extreme example, if you have a team where everyone still uses FORTRAN because that’s what they are used to, it would be unfortunate to hire a person who understands Python (and could help bridge the team to a more modern technology stack) and encourage them forget Python and learn FORTRAN.\nIn the other scenario, where a former student tries to move the organization towards the newer technologies themselves, an immense amount of chaos can inadvertently be created. These sorts of situations generally arise when the former student thinks they have a better way to do something than the existing methods. For example, maybe an organization has critical data stored on a single Microsoft Access database under someone’s desk. The student might (correctly) point out that this is dangerous, and suggest migrating the data to a cheap cloud database. A change like this might sound straightforward, but a lot of work has to be done in shifting to a new technology, including carefully considering questions like:\n\nWho on the team has the skill set to use this?\nHow will it be maintained if the creator leaves the team?\nHow does this align with the technology choices of other parts of the organization?\n\nAre all critically important to the team’s implementation and enduring success with a new technology. Meanwhile, people who only recently entered the working world have the least perspective on these challenging questions. In practice what often happens is the new hire disregards those challenges, and instead implements what they learned in school without enough consideration for how it will be adopted, used and maintained by coworkers. Replacing a dated but well-understood technology with new technology that few can maintain can be far worse than keeping what was there before. A team may be left with something that is now more complicated for them to use and maintain, and no one who knows how to do it. A functioning system needs a robust set of measures in case of failures, and a single recent graduate new to the field cannot provide that on their own.\n\n\n\nReplacing a dated but well-understood technology with new technology that few can maintain can be far worse than keeping what was there before.\n\n\n\nA former student trying to get a new technology adopted in an organization may also run into communication issues and conflicts when trying to make change in the organization. If the organization doesn’t see the value in the new technologies (why use git when SharePoint is fine?) people may become frustrated that the new hire is bringing them up. When you join a new organization, especially having only recently become a data scientist, you rarely have much political capital, and it’s very easy to spend that capital in places that ultimately don’t prove useful. So this scenario where the recent hire tries to get a new technology adopted can leave a person frustrated and without tools to move forward in an organization.\n\n\n\nJacqueline sidebar\n\n\nGoing into industry, I absolutely fell into the second scenario at multiple companies. Repeatedly I would introduce a technology that the rest of the team wasn’t familiar with, like using MATLAB while everyone else used SAS or R when people used Excel. I had valid reasons for believing why the technology I was introducing was better, but I failed to account for the reality that I would be the only one who used them. Anytime something broke I would have to be the one who fixed it and it was hard for my teammates to champion the work I did. In almost every situation I eventually ended up redoing my work in the tool the team used. Now that I am substantially more senior I am able to transition teams to newer technologies, but it takes a much deeper understanding of how organizations operate, and the fact that I have senior positions to influence from, to bring the change.\n\n\n\n\n\nWhat we need to incorporate this into data science programs\nTo reduce the potential conflicts we’ve described, preparing students in strategies and mindsets to merge their skills into existing teams should be part of data science programs. Teachers familiar with tools used in academia, however, may not have their finger on the current pulse of how data science is done in other sectors (industry, government, NGOs, non-profits, etc.). One effective way to keep knowledge flowing to academia and students is for faculty to help bridge more connections between industry and academia.\nTeachers in data science degree programs should familiarize themselves with the current landscape of data science tools and systems used by the types of teams they expect their students to join. That might include:\n\nEngaging with industry / non-academic partners about how they work with data\nSearching for and synthesizing skills in data science job postings\nCommunicating with program alumni about how they are working post-graduation\n\nThis will likely require teachers to step out of their comfort zone and communicate with people they don’t normally talk to. But even one connection between an academic and a person working in industry can vastly improve how much the academic understands about industry.\nTeachers should also facilitate opportunities for students to work with and/or learn from non-academics in similar positions or workplaces, such as:\n\nCourse projects, capstone & thesis projects that require students to integrate their skills into existing team workflows\nData science summer internships and part time jobs\nWorkshops, case studies, guest speakers, etc. to build skills on the human side of working with teams\n\nMany data science programs already have a client-proposed capstone project as part of their requirements. For example, the University of British Columbia Master of Data Science, UC Santa Barbara Master of Environmental Data Science, and University of Washington Data Science Master’s program and others require students to work in groups to solve a problem while working with an (often external) client. Capstone advisers and teachers should ensure that students are keeping the clients’ existing tools and workflows at front of mind so that students create a deliverable that is understandable, usable, and maintainable by people within the organization they’re making it for.\nThese sorts of opportunities are great for students–not only do they give students a better understanding of what industry is like, but they also make students much more appealing on the job market. If teachers have industry connections then they can be used to help connect students too. Academic organizations are also starting to recognize how important these connections can be. One great example is PIC Math (run by the Mathematical Association of America) which helps professors start learning how to start working with industrial sponsors and provides ways for students to do industry projects.\nWhile connecting better with industry will help solve this problem, it’s certainly not the only method that could prove useful. This problem of overly equipped students is a common enough occurrence that academics and students should be well aware of it and as a field we should strive to make progress on it in the years ahead."
  },
  {
    "objectID": "posts/2019-11-19-rstudio-artist-in-residence/index.html",
    "href": "posts/2019-11-19-rstudio-artist-in-residence/index.html",
    "title": "RStudio Artist-in-Residence 2019/2020!",
    "section": "",
    "text": "I am extremely excited to share that as of October 2019, I am RStudio’s first Artist-in-Residence!\n\nYou can read more about my motivation and goals for the position on the RStudio blog.\nI look forward to sharing a whole bunch more art with you over the next year!\nAllison"
  },
  {
    "objectID": "posts/2019-04-29-share-keys/index.html",
    "href": "posts/2019-04-29-share-keys/index.html",
    "title": "Sharing (R lab) keys for success",
    "section": "",
    "text": "I just finished my sixth year of teaching intro stats and data analysis in R to environmental studies grad students. For the first five, I convinced myself that I shouldn’t share my instructor code keys with students before our weekly #rstats labs.\nHere are a couple of my commonly regurgitated excuses, based on absolutely nothing:\nThis year, a student was struggling to keep up in our computer labs and asked to get the instructor keys beforehand. Labs progress quickly, and they wanted to explore the the key to identify challenging/confusing chunks and prepare for them in advance.\nFirst I was like “eeeehhhhh,” then I was like “pssshh fine I’ll email it to you but please don’t share it with anybody else because [see excuses above],” then eventually I was like “this is ridiculous let’s see what happens if I share the instructor keys beforehand with everyone in the class.”\nHere is the nightmare that ensued:"
  },
  {
    "objectID": "posts/2019-04-29-share-keys/index.html#everyone-just-still-came-to-labs.",
    "href": "posts/2019-04-29-share-keys/index.html#everyone-just-still-came-to-labs.",
    "title": "Sharing (R lab) keys for success",
    "section": "1. Everyone just still came to labs.",
    "text": "1. Everyone just still came to labs.\nSeriously, attrition rate = zero."
  },
  {
    "objectID": "posts/2019-04-29-share-keys/index.html#everyone-was-still-coding-in-labs.",
    "href": "posts/2019-04-29-share-keys/index.html#everyone-was-still-coding-in-labs.",
    "title": "Sharing (R lab) keys for success",
    "section": "2. Everyone was still coding in labs.",
    "text": "2. Everyone was still coding in labs.\nMy sense was that most students didn’t even look at the key despite knowing it was posted (also fine). But in each lab, several students had printed out the key as an additional resource to help them along. In short: self-motivated students who benefitted from exploring lab keys beforehand could, and the rest just continued showing up and following along without it like they always had.\nThis year, I actually found no down side to posting my instructor code keys for students and TAs to see in advance. I’m sure there are anecdotal horror stories of shared keys … FINISH THIS"
  },
  {
    "objectID": "posts/2019-04-29-share-keys/index.html#in-fact-there-are-clear-benefits.-like",
    "href": "posts/2019-04-29-share-keys/index.html#in-fact-there-are-clear-benefits.-like",
    "title": "Sharing (R lab) keys for success",
    "section": "In fact, there are clear benefits. Like:",
    "text": "In fact, there are clear benefits. Like:\n\nIf students want to spend more time looking over code before labs, why in the world would I ever not encourage their interest and effort?\nWhen students have to miss labs (illness, personal days, conferences, etc.) they know that the materials are all available without request - which means less stress and emailing for everyone.\nFor anyone who does find themselves falling behind, there’s always a key available for them to figure out where they got lost or went awry. Another small way to reduce student stress.\nIt’s actually less work for me. I can just push all materials to GitHub (data, lab templates, keys), without feeling like I have to hide pieces in different repos or folders or whatever. One (openly shared) Rproj to rule them all."
  },
  {
    "objectID": "posts/2019-04-29-share-keys/index.html#what-does-this-require",
    "href": "posts/2019-04-29-share-keys/index.html#what-does-this-require",
    "title": "Sharing (R lab) keys for success",
    "section": "What DOES this require?",
    "text": "What DOES this require?\nBelieving in students.\nWhen it comes right down to it, I wasn’t sharing instructor keys because I didn’t give my students the credit they deserved. Which was really lame of me, because like 99.9% of the time they’ve only ever been more committed and motivated than I expect them to be.\nAnyway, the students asking to review the lab materials beforehand are probably not the ones trying to weasel their way out of course effort.\nI’ll continue posting all #rstats lab keys (heck, you can even see them: ESM 206 / ESM 244), with gratitude to students who’ve repeatedly proven my reservations wrong."
  },
  {
    "objectID": "posts/2019-05-02-tidy-tuesday-coding-club/index.html",
    "href": "posts/2019-05-02-tidy-tuesday-coding-club/index.html",
    "title": "Starting our #tidytuesday hacky hours",
    "section": "",
    "text": "This is our first community blog post by Openscapes Champion Allison Horst!\n\nHi everyone! I’m Allison. I teach data analysis, statistics and presentation skills to graduate students at the Bren School of Environmental Science and Management at UC Santa Barbara. I’m also an Openscapes Champion.\nIn Openscapes we discuss the need to create supportive spaces, like coding clubs, for useRs to practice and grow their coding skills.\nWith Bren masters student Gracie White (@GracieGW), I’ve recently started a weekly hacky hour / coding club in Santa Barbara. Here, I’ll share how #tidytuesday has reduced our workload and planning, how we started and organize our events, and a few things we’ve learned so far.\n\n\n \n\n\n\n\nComing off of two quarters of data analysis courses, a number of motivated students were looking for a stress-free way to maintain and grow their coding skills. Recognizing the momentum, Gracie reached out requesting that we start a coding club this quarter using RStudio’s #tidytuesday initiative as an impetus and opportunity for weekly social coding events.\nSo, what is #tidytuesday? As described by Thomas Mock (@thomas_mock) of RStudio and the R for Data Science (#R4DS) online learning community:\n\n“Every week we post a raw dataset, a chart or article related to that dataset, and ask you to explore the data…The goal of TidyTuesday is to apply your R skills, get feedback, explore other’s work, and connect with the greater #RStats community!”\n\nWhy is that so great?\nBecause it means that the groundwork for a short, productive session of data wrangling practice already exists, with very little effort on our end. Manageable #tidytuesday datasets are shared each week with background information, citations, and code to read them directly into R.\nAttendees are invited to bring any other code-related projects they’re working on, but having the #tidytuesday option means that there’s something for everyone to work on that week, and we don’t have to spend any energy to make it available or accessible. Another bonus: many #tidytuesday participants share their code and visualizations publicly, so anyone can explore examples to learn from what others have done!\n\n\n \n\n\n\n\n\n\n\nOnce we knew we’d focus our sessions around #tidytuesday datasets, we welcomed people to join us. In our first email, we wanted to:\n\nWelcome people and share our enthusiasm\nInclude the date, time and location of our first event\nIntroduce the #tidytuesday initiative\nAsk interested people to add their name to our Google sign-up sheet\n\nWe sent out the following email to Bren students and other R-using friends in the UCSB community:\n\nI’m very excited to share that we’re starting a Tidy Tuesday Coding Club next quarter. Our hope is to make this a fun, no-stress, and supportive weekly group where we can all work on our own coding projects, discuss challenges, share victories (and grievances), learn about cool new packages and functions, and keep the rust off of our R coding skills!\n\n\nOur first coding club will be DATE, TIME, at LOCATION. We plan to schedule a place each Tuesday (e.g. coffee shops, breweries, restaurants, etc.) where we can meet in the afternoon/evening and code together. You can bring your own code/data you’re working on, or participate in the “R for Data Science” online community’s #TidyTuesday initiative, summarized here:\n\n\n“Join the R4DS online learning community in the weekly #TidyTuesday event! Every week we post a raw dataset, a chart or article related to that dataset, and ask you to explore the data. While the dataset will be “tamed”, it will not always be tidy! As such you might need to apply various R for Data Science techniques to wrangle the data into a true tidy format. The goal of TidyTuesday is to apply your R skills, get feedback, explore other’s work, and connect with the greater #RStats community! As such we encourage everyone of all skills to participate!“\n\n\nWe’ll be participating in #TidyTuesday each week, and we hope you’ll join us for some fun social coding (yes, those can coexist). Sign-up HERE to be included on future emails about the dates/times/locations of our Tidy Tuesday Coding Club. We look forward to seeing you there!\n\n\n\n\nWe held our first event at a cool little brewery close to campus that satisfied some requirements:\n\nEasy to bike or bus from campus\nPlenty of seating and tables for computers\nFree and reliable wifi\nFood & refreshments available (at venue or very close by)\nNo loud events happening (e.g. trivia night, live bands, etc.)\n\n…and ~17 coders came out to join us!\n\n\n\n\n\nThere was a pretty even mix of attendees working on their own projects (research, assignments, etc.) and those using #tidytuesday to practice wrangling and visualization.\nOne person remembered how (and why!) to work in R projects. Several of us re-learned a few things about the lubridate package for working with dates and times. For a few, even opening RStudio and reading in data after a long hiatus felt victorious.\nI was thrilled to be there with so many people who felt comfortable working alongside other R-users at all different levels. It was a fun, relaxed evening of coding together. At subsequent events we’ve had between 12 and 18 people (and at least one dog), which is perfect for the locations we’ve met so far, and new interested members continue adding their names to our sign-up sheet.\n\n\n\nThe Wednesday or Thursday following each event, I send an email to the group with the time and location of our next meeting. I also include some code examples inspired by discussions or data wrangling efforts during that week’s coding club (and am adding them to a GitHub repo).\nMonday or Tuesday after the new #tidytuesday datasets are posted, Gracie sends an email welcoming people to join us that week with a short summary of that week’s data topic and code to read in the datasets directly.\nSo at this point, our effort as organizers is pretty minimal:\n\nMonday/Tuesday: Gracie sends reminder email + code to get #tidytuesday data\nTuesday: We attend our coding club (2 hours/wk)\nWednesday: I post my code examples on GitHub (takes ~30 minutes)\nWednesday/Thursday: I send a follow-up with next location + links to code examples\n\nFor now, that’s our low-effort, big-reward strategy for starting a social coding group!\n\n\n\n\n\n\n\n\n\nSome lessons we’ve learned very early on in our little coding club’s existence:\n\nYou don’t need a perfect plan to start a coding club. I’d been so worried about details that I delayed starting a coding club. I shouldn’t have. In our first few weeks I’ve learned that attendees are happy to participate in less structured, low-key social coding events - and that I don’t need to feel like a host every week.\nUse materials that others have already created to minimize planning effort. The goal of our club is to get and keep everyone coding frequently so that we can all maintain and grow our R-skills. For that purpose, using the existing #tidytuesday infrastructure makes a lot of sense.\nWifi is really important. As we learned last week, our sessions are way less productive if we can’t easily and quickly search examples, twitter, blogs, documentation, forums, etc. We’ve had to cross a couple of places off our locations list due to unreliable wifi.\nYou don’t need an expert in attendance, but it helps to have some people who can help with issues when people get stuck. In our group, most people have had 1 - 2 recent courses in R and are relatively independent when it comes to the basics. Still, it’s useful to have one or two people there who feel comfortable troubleshooting or can send people in the right direction.\nKnow your audience, and choose your format accordingly. Our “get together and code socially” format works for our group because we have a critical mass of semi-independent R-users who are able to work and learn at a level that makes sense for #tidytuesdays (basic data wrangling + visualization with the tidyverse). If that isn’t the case, another format (e.g. a coding club where attendees work through tutorials together, or where an instructor leads a short workshop on a different topic like in eco-data-science) might be more productive. Figure out the goals of your coding club first, try to anticipate the level of most attendees, and choose a format that will set them up to meet those goals.\n\nWe’re really excited about our young coding club so far, and will share what we learn as it continues to grow!"
  },
  {
    "objectID": "posts/2020-08-01-palmerpenguins-on-cran/index.html",
    "href": "posts/2020-08-01-palmerpenguins-on-cran/index.html",
    "title": "palmerpenguins on CRAN",
    "section": "",
    "text": "Learn more!\nRead our post on the RStudio Education blog: https://education.rstudio.com/blog/2020/07/palmerpenguins-cran/\nVisit our package website for more information: https://allisonhorst.github.io/palmerpenguins/\nFind them on CRAN: https://cran.r-project.org/web/packages/palmerpenguins/index.html\nHave fun with the palmerpenguins!"
  },
  {
    "objectID": "posts/2021-02-08-dplyr-learnr/index.html",
    "href": "posts/2021-02-08-dplyr-learnr/index.html",
    "title": "Penguin wrangling in dplyr - a learnr tutorial",
    "section": "",
    "text": "https://allisonhorst.shinyapps.io/dplyr-learnr/\nHere’s the GitHub repo in case anyone wants to modify/update this for your own courses:\nhttps://github.com/allisonhorst/dplyr-learnr\nHave fun wrangling penguins!"
  },
  {
    "objectID": "posts/2022-03-12-shrinking-glaciers/index.html",
    "href": "posts/2022-03-12-shrinking-glaciers/index.html",
    "title": "Losing icons: Disappearing glaciers of Glacier National Park, MT (1966 - 2015)",
    "section": "",
    "text": "Data visualization - glaciers at Glacier National Park (1966, 1998, 2005, and 2005).\nData: Fagre, D.B., McKeon, L.A., Dick, K.A., and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release, https://doi.org/10.5066/F7P26WB1.\nAttach libraries:\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(here)\nlibrary(cowplot) # For the plot grid\nlibrary(janitor) # For cleaning names\nlibrary(showtext) # For font changes\nlibrary(ggforce) # For geom_circle() in key\nlibrary(grid) # Add line segments for key\n\n# Import font\nfont_add_google(name = \"Overpass\", family = \"overpass\") # add custom fonts\nfont_add_google(name = \"Nixie One\", family = \"nixie\")\nshowtext_auto()\n\n\nRead in the data:\n\n\nCode\nglaciers_1966 <- read_sf(here::here(\"data\", \"GNPglaciers_1966\")) %>% \n  janitor::clean_names() %>% \n  mutate(glacname = word(glacname , 1  , -2))\nglaciers_1998 <- read_sf(here::here(\"data\", \"GNPglaciers_1998\"))%>% \n  janitor::clean_names() %>% \n  mutate(glacname = word(glacname , 1  , -2))\nglaciers_2005 <- read_sf(here::here(\"data\", \"GNPglaciers_2005\"))%>% \n  janitor::clean_names() %>% \n  mutate(glacname = word(glacname , 1  , -2))\nglaciers_2015 <- read_sf(here::here(\"data\", \"GNPglaciers_2015\"))%>% \n  janitor::clean_names() %>% \n  mutate(glacname = word(glacname , 1  , -2))\n\n\nMake the plot:\n\n\nCode\n# Set colors for different years:\ncol_1966 <- \"#7bb0ae\"\ncol_1998 <- \"#a2c8c6\"\ncol_2005 <- \"#cfe7e5\"\ncol_2015 <- \"white\"\n\nglaciers <- purrr::map(glaciers_1966$glacname,\n                       function(x) {\n                         ggplot() +\n                           geom_sf(data = filter(glaciers_1966, glacname == x), \n                                   fill = col_1966, color = NA) +\n                           geom_sf(data = filter(glaciers_1998, glacname == x),\n                                   fill = col_1998, color = NA) +\n                           geom_sf(data = filter(glaciers_2005, glacname == x),\n                                   fill = col_2005, color = NA) +\n                           geom_sf(data = filter(glaciers_2015, glacname == x),\n                                   fill = col_2015, color = NA) +\n                           ggtitle(x) +\n                           theme_void() +\n                           theme(title = element_text(size = 40, \n                                                      color = \"white\", \n                                                      family = \"overpass\"\n                                                      )\n                                 )\n})\n\n\nglacier_grid <- cowplot::plot_grid(plotlist = glaciers) +\n  annotate(\"text\", \n           label = \"Shrinking glaciers\", \n           x = 0.02, y = 1.1,\n           color = \"white\",\n           family = \"nixie\",\n           size = 55,\n           hjust = 0,\n           fontface = \"bold\") +\n  annotate(\"text\",\n           label = \"Glacier National Park, Montana (1966 - 2015)\",\n           color = \"white\",\n           x = 0.02, y = 1.05,\n           family = \"nixie\",\n           size = 30,\n           hjust = 0) +\n  geom_circle(aes(x0 = 0.696, y0 = 0.09, r = 0.05), \n              fill = col_1966, color = NA) +\n  annotate(\"text\", label = \"1966 extent\", \n           color = col_1966, \n           x = 0.81, y = 0.130, \n           size = 20, \n           hjust = 0, \n           family = \"overpass\") + \n  annotation_custom(grob = linesGrob(gp = gpar(col = col_1966, lwd = 2)), \n                    xmin = 0.70, xmax = 0.80, ymin = 0.130, ymax = 0.130) +\n  geom_circle(aes(x0 = 0.71, y0 = 0.08, r = 0.03), \n              fill = col_1998, color = NA) +\n  annotate(\"text\", label = \"1998 extent\", \n           color = col_1998, \n           x = 0.81, y = 0.108, \n           size = 20, \n           hjust = 0, \n           family = \"overpass\") + \n  annotation_custom(grob = linesGrob(gp = gpar(col = col_1998, lwd = 2)), \n                    xmin = 0.71, xmax = 0.80, ymin = 0.108, ymax = 0.108) +\n  geom_circle(aes(x0 = 0.718, y0 = 0.076, r = 0.02), \n              fill = col_2005, color = NA) +\n  annotate(\"text\", label = \"2005 extent\", \n           color = col_2005, \n           x = 0.81, y = 0.087, \n           size = 20, \n           hjust = 0, \n           family = \"overpass\") + \n  annotation_custom(grob = linesGrob(gp = gpar(col = col_2005, lwd = 2)), \n                    xmin = 0.735, xmax = 0.80, ymin = 0.087, ymax = 0.087) +\n  geom_circle(aes(x0 = 0.725, y0 = 0.073, r = 0.01), \n              fill = col_2015, color = NA) +\n  annotate(\"text\", label = \"2015 extent\", \n           color = col_2015, \n           x = 0.81, y = 0.065, \n           size = 20, \n           hjust = 0, \n           family = \"overpass\") +\n  annotation_custom(grob = linesGrob(gp = gpar(col = col_2015, lwd = 2)), \n                    xmin = 0.730, xmax = 0.80, ymin = 0.065, ymax = 0.065) +\n  annotate(\"text\", label = \"Data: Fagre et al, 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release, https://doi.org/10.5066/F7P26WB1.\", \n           x = 0.04, y = -0.03, \n           size = 10, \n           hjust = 0, \n           color = col_2005) +\n  annotate(\"text\", label = \"@allison_horst\", \n           x = 0.91, y = -0.06, \n           size = 14, \n           hjust = 0, \n           color = col_2005, \n           family = \"nixie\") +\n  annotate(\"text\", label = \"Note: glaciers not to scale\", \n           x = 0.74, y = 0.04, \n           size = 15, \n           hjust = 0, \n           color = col_2005, \n           fontface = \"italic\") +\n  theme(plot.background = element_rect(fill = \"#192c30\", colour = NA),\n          plot.margin = margin(85, 20, 40, 20))\n\nggsave(plot = glacier_grid, \"glaciers.png\", width = 7, height = 8, dpi = 600)\nggsave(plot = glacier_grid, \"featured_glaciers.png\", width = 7, height = 8, dpi = 600)"
  },
  {
    "objectID": "posts/2022-10-14-bird-attacks/index.html",
    "href": "posts/2022-10-14-bird-attacks/index.html",
    "title": "1974…bird attacks?",
    "section": "",
    "text": "Gotcha.\nAround 11pm last night I saw this tweet:\n\n\nTom’s instinct makes perfect sense. My first thought was “CHICKEN WINGS.” So I spent a couple sleepless hours with this weird (and obviously fake) data set about bird attacks in 1974, because it felt like a good October distraction and an opportunity to get some data wrangling rust off.\nIn this post:\n\nImage cropping with magick\nOptical character recognition (OCR) with tesseract\nLow elegance, no shame data tidying in R\nStatic & interactive visualizations in JavaScript with Plot & imported D3 charts"
  },
  {
    "objectID": "posts/2022-10-14-bird-attacks/index.html#get-the-tools-and-data",
    "href": "posts/2022-10-14-bird-attacks/index.html#get-the-tools-and-data",
    "title": "1974…bird attacks?",
    "section": "Get the tools and data:",
    "text": "Get the tools and data:\nAttach libraries:\n\nlibrary(magick)\nlibrary(tesseract)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(kableExtra)\n\nRead in and crop the image with magick:\n\nbirds_img <- image_read(\"bird_attacks_tweet.jpg\")\nbirds <- image_crop(birds_img, \"1100x1100+50+200\")\n\nHere’s the cropped version of the image:"
  },
  {
    "objectID": "posts/2022-10-14-bird-attacks/index.html#optical-character-recognition",
    "href": "posts/2022-10-14-bird-attacks/index.html#optical-character-recognition",
    "title": "1974…bird attacks?",
    "section": "Optical character recognition",
    "text": "Optical character recognition\nOptical character recognition (OCR), or text recognition, lets you extract text information from images (e.g. non-selectable PDFs, JPEGs, PNGs, etc.). The tesseract package in R “provides R bindings to Tesseract: a powerful optical character recognition (OCR) engine that supports over 100 languages.”\nCreate the engine, then OCR those birds:\n\nocr_eng <- tesseract(\"eng\")\ntext <- tesseract::ocr(birds, engine = ocr_eng)\n\nTake a look at the extracted text. I will never stop thinking this is amazing.\n\ncat(text)\n\nBIRD ATTACK STATISTICS 1974\nTable 1. Bird attacks: Table 2. Bird attacks:\nbody parts injured nature of wounds\nCalf/knee 10 Severe lacerations 46\nThigh 14 Significant loss of tissue 37\nEyes 985 Bone exposed 12\nArms 25 Scalping 21\nFeet 0 Good old-fashioned\nHair extensions 5 disemboweling 2\nPancreas 4 Waste burns 31\nButtocks 99 Raking 4\nFingers 86 Cut by stinging remarks 7\nAbdomen/stomach 9 Body skeletonized 52\nHead 54 Appendage lost to bird 17\nCankles 293 Lobotomy through nostril 8\nBack 8 Feelings hurt 42\nCellulite 210 Swallowed whole or\nGenitals 834 presumed so 16\n\n(Body not recovered in 7 cases.)"
  },
  {
    "objectID": "posts/2022-10-14-bird-attacks/index.html#cleaning",
    "href": "posts/2022-10-14-bird-attacks/index.html#cleaning",
    "title": "1974…bird attacks?",
    "section": "Cleaning",
    "text": "Cleaning\nHere’s my “no regerts” code for data cleaning. I would love to see other ways that people wrangle these birds.\n\n# Separating into columns\nbird_dat <- data.frame(text) |> \n  mutate(text = strsplit(as.character(text), \"\\n\")) |>  \n    unnest(text) |> \n  filter(text != \"\") |>\n  slice(-c(1:2)) |> \n  separate(col = \"text\" , \n           into = c(\"body_parts\", \"leftover\"), \n           sep = \"(?<=[a-zA-Z])\\\\s*(?=[0-9])\",\n           extra = \"merge\") |> \n  separate(col = \"leftover\",\n           into = c(\"body_parts_count\", \"leftover\"),\n           sep = \" \",\n           extra = \"merge\") |> \n  separate(col = \"leftover\" , \n           into = c(\"nature_of_wounds\", \"nature_of_wounds_counts\"), \n           sep = \"(?<=[a-zA-Z])\\\\s*(?=[0-9])\",\n           extra = \"merge\") |> \n  slice(-1)\n\n# Some artisanal hacking. Not proud, but tired... \nbird_dat$nature_of_wounds[16] <- \"Body not recovered\"\nbird_dat$nature_of_wounds_counts[16] <- bird_dat$body_parts_count[16]\nbird_dat$body_parts[16] <- NA\nbird_dat$body_parts_count[16] <- NA\n\n# At this point, splitting into separate data frames:\nbody_parts_injured <- data.frame(body_parts = bird_dat$body_parts, n = bird_dat$body_parts_count) |> \n  drop_na()\n\nwounds <- data.frame(wound_nature = bird_dat$nature_of_wounds, \n                     n = bird_dat$nature_of_wounds_counts)\n\n# Where nature_of_wounds_counts is NA, combine text w/ following row.\n# No, this does not follow any cohesive data cleaning philosophy.\nfor (i in 1:(length(wounds$n) - 1)) {\n  if (is.na(wounds$n[i])) {\n    wounds$wound_nature[i+1] <- paste(wounds$wound_nature[i], wounds$wound_nature[i+1])\n    }\n}\n\n# Then get rid of the duplicate text rows still containing NA\nwounds <- wounds |> \n  drop_na() |> \n  mutate(n = as.numeric(n))\n\nWhich gives us these two data frames:\n\n\n\n\n\n \n  \n    body_parts \n    n \n  \n \n\n  \n    Calf/knee \n    10 \n  \n  \n    Thigh \n    14 \n  \n  \n    Eyes \n    985 \n  \n  \n    Arms \n    25 \n  \n  \n    Feet \n    0 \n  \n  \n    Hair extensions \n    5 \n  \n  \n    Pancreas \n    4 \n  \n  \n    Buttocks \n    99 \n  \n  \n    Fingers \n    86 \n  \n  \n    Abdomen/stomach \n    9 \n  \n  \n    Head \n    54 \n  \n  \n    Cankles \n    293 \n  \n  \n    Back \n    8 \n  \n  \n    Cellulite \n    210 \n  \n  \n    Genitals \n    834 \n  \n\n\n\n\n\n\n\n \n  \n    wound_nature \n    n \n  \n \n\n  \n    Severe lacerations \n    46 \n  \n  \n    Significant loss of tissue \n    37 \n  \n  \n    Bone exposed \n    12 \n  \n  \n    Scalping \n    21 \n  \n  \n    Good old-fashioned disemboweling \n    2 \n  \n  \n    Waste burns \n    31 \n  \n  \n    Raking \n    4 \n  \n  \n    Cut by stinging remarks \n    7 \n  \n  \n    Body skeletonized \n    52 \n  \n  \n    Appendage lost to bird \n    17 \n  \n  \n    Lobotomy through nostril \n    8 \n  \n  \n    Feelings hurt \n    42 \n  \n  \n    Swallowed whole or presumed so \n    16 \n  \n  \n    Body not recovered \n    7"
  },
  {
    "objectID": "posts/2022-10-14-bird-attacks/index.html#viz-those-birds",
    "href": "posts/2022-10-14-bird-attacks/index.html#viz-those-birds",
    "title": "1974…bird attacks?",
    "section": "Viz those birds",
    "text": "Viz those birds\nI’m going to do some JavaScript data visualization in OJS cells, which I can do right in this Quarto doc 🤯 Curious about adding more JavaScript to your Quarto world? I highly recommend Sharon Machlis’ recent series, A beginner’s guide to using Observable JavaScript, R, and Python with Quarto.\nKeep in mind that you can’t interactively execute JavaScript in RStudio, so building visualizations in Observable is a great option.\nFirst, I’ll make my R objects available for use in OJS cells:\n\nojs_define(wounds = wounds)\nojs_define(body_parts_injured = body_parts_injured)\n\nAt this point, I switch over to OJS cells – code fencing is shown for OJS cells below to clarify the switch from R. First step, I’ll make transposed versions of the data frames:\n\n```{ojs}\n//| echo: fenced\nwounds_transpose = transpose(wounds)\ninjuries_transpose = transpose(body_parts_injured)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThen some JavaScript visualizations.\nHere, a bar chart created with Observable Plot. If you’re used to working in ggplot2, building visualizations following the grammar of graphics in Plot feels similar.\nPlot accepts columnar (untransposed) data (thanks Fil!), as shown below. Notice that the first argument provided to Plot.barX() is wounds.n. Why not just wounds? Here, we need to provide an argument that has the correct length.\n\n```{ojs}\n//| echo: fenced\n\nPlot.plot({\n    marks: [\n            Plot.barX(wounds.n, \n                     {y: wounds.wound_nature, x: wounds.n,\n                     sort: {y: \"x\", reverse: true},\n                     fill: \"navy\"}\n                     )\n           ],\n    marginLeft: 200,\n    x: {label: \"Number\"},\n    y: {label: \"Type of injury from bird attack\"}\n  })\n```\n\n\n\n\n\n\nAlternatively, we can pass our transposed data into Plot to make the same thing. This works just fine here, with a small data set. Once the number of rows gets very large, you may want to avoid transposition, and can use the “direct from columns” method above.\n\n```{ojs}\n//| echo: fenced\nPlot.plot({\n  marks: [\n    Plot.barX(wounds_transpose, \n    {y: \"wound_nature\", x: \"n\", \n    sort: {y: \"x\", reverse: true},\n    fill: \"navy\"\n    })\n  ], \n  marginLeft: 200, \n  x: {label: \"Number\"},\n  y: {label: \"Type of injury from bird attack\"}\n})\n```\n\n\n\n\n\n\nWe know this data is questionable because clearly the two most common bird wounds are “Cut by stinging remarks” and “Feelings hurt.” I suspect this may be due to social desirability bias. Further research is needed.\nYou know what viz I really need (read: I really don’t, but keep reading because this is cool anyway)? A D3 bubble chart of those wounds, with circle area proportional to counts. Instead of building this from scratch, I’m going to use Observable imports to have it (or any other named thing in a published Observable notebook) at-my-fingertips. Again, this feels like a bit of magic.\n\n```{ojs}\n//| echo: fenced\nimport {BubbleChart} from \"@d3/bubble-chart\"\n```\n\n\n\n\n\n\nNow, the D3 BubbleChart function is available for me to usehere. Right in my Quarto doc. I’m clapping.\n\n```{ojs}\n//| echo: fenced\nBubbleChart(wounds_transpose, \n            {label: (d) => `${d.wound_nature}\\n\\n${d.n}`,\n            value: d => d.n,\n            group: d => d.n,\n            title: (d) => `${d.wound_nature}\\n\\n${d.n}`\n            })\n```\n\n\n\n\n\n\nI refuse to leave this without some visualization of the “body parts injured” data. Let’s make a (spin the wheel) D3 DONUT CHART!\nAgain, I import from th Observable Notebook (notice the slug is @d3/donut-chart, and the named function is DonutChart):\n\nimport {DonutChart} from \"@d3/donut-chart\"\n\n\n\n\n\n\nWith the DonutChart function now available for me to use here, I can make my plot:\n\nDonutChart(injuries_transpose, {\n  name: d => d.body_parts,\n  value: d => d.n,\n  width,\n  height: 500\n})\n\n\n\n\n\n\nIn the spirit of publicly sharing works-in-progress and imperfect things, I’ll stop here. I look forward to seeing other ways that people wrangle & visualize data from the 1974 bird attack epidemic. Stay safe out there, friends.\n\nJSON.stringify(wounds)"
  },
  {
    "objectID": "posts/2021-09-02-learning-github-classroom/index.html",
    "href": "posts/2021-09-02-learning-github-classroom/index.html",
    "title": "Learning GitHub Classroom",
    "section": "",
    "text": "Recently, a small group of us at the Bren School affiliated with our new Master of Environmental Data Science program decided to learn GitHub Classroom together. We wrote about our experience and takeaways for the Academic Data Science Alliance Community Blog. Ready it here!"
  },
  {
    "objectID": "posts/2020-02-29-birds-of-a-feather-drawn-together/index.html",
    "href": "posts/2020-02-29-birds-of-a-feather-drawn-together/index.html",
    "title": "Birds of a feather, drawn together",
    "section": "",
    "text": "Cross-posted:\n\n\n\nBirds of a Feather banner for RStudio::conf(2020)!\n\n\nThe “Birds of a Feather” (BoF) sessions at the 2020 RStudio Conference were a place where R-users with similar backgrounds, interests, and aspirations could connect in a low-stress social setting. In other words - these sessions let “birds of a feather flock together.” Building on the 2019 conference swag, we rolled out a bunch of new BoF buttons for rstudio::conf 2020.\nHere we’ll share the process for how some of the designs were created, including several that truly involved a flock of contributors (names for groups of birds are from Bird Spot).\n\nWell, what can we realistically fit on a 1.5” button?\nThe cool and challenging thing about designing the BoF buttons was the size restriction. In a 1.5” button, we wanted to create designs that:\n\nReflect the group name and/or community\nInclude at least one bird\nAre obviously distinguishable for each group\nAdd some readable text\nOh also, all while setting an approachable tone.\n\nIt turns out the answer to “What can you fit on a 1.5” button?” is…actually quite a lot. Here are a few of the designs:\n\n\nData science team leaders\nFeaturing 80’s beach volleyball sandpiper coach leading a team of R users:\n\n\n\nFun fact: a group of sandpipers can be called a fling or a bind.\n\n\nNatural language processing\nFeaturing a wordy bird - a parrot!\n\n\n\nFun facts: Groups of parrots are sometimes called companies, a prattles, or pandemoniums. Also, parrots are the only birds that can use their feet (like hands) to serve themselves food. And one more thing - the text in the background of this design is a line from Patrick Rothfuss’ The Name of the Wind (one of artist @allison_horst’s favorite books).\n\n\nThe spatially aware carrier pigeon…\n…checking flight routes before setting off on their next delivery:\n\n\n\nFun fact: A group of pigeons is sometimes called a kit or a loft.\n\n\nCommunity-driven designs\nSeveral of the designs - especially R-Latin America, R-Africa, and R-Ladies - depended on recommendations, insights, and feedback from multiple members of those communities. Here, we’ll highlight elements and contributors of several community-driven designs.\n\nR-Latin America\n\n\n\nThe R-Latin America BoF button was truly a collaborative effort, with every detail - from bird to branch - created based on ideas and input from R-Latin America community members. Massive thanks to the R-Latin America design contributors (and thank you to @yabellini for providing this compiled list & text):\nLaura Ación, Marcela Alfaro, Yanina Bellini, Juliana Benitez (expert ornithologist!), Elio Campitelli, Paola Corrales, Florencia D’Andrea, Natalia da Silva, Priscilla Minotti; Riva Quiroga, Vilma Romero, Gabriela Sandoval, Heather Turner; and the LatinR Conference Community!\nThese contributors are from Argentina, Chile, Costa Rica, Perú, Uruguay and the UK, and belong to R-Ladies Chapters and R User Groups in these countries. The final picture was chosen for the vote of the broader LatinR Conference Community.\nHere are some insights into the final design elements:\n\nThe bird species is the chingolo (Zonotrichia capensis) which was chosen because it seemed to be the bird that is native to many Latin American countries, and is also unique to Latin America. It was one of a number suggested by ornithologist Juliana Benitez!\nThe chingolo is standing on a tilde (“~”) branch, used in letter eñe (ñ)\nThe color bands on the tilde represent colors on flags for Latin American countries\nThe shape of Latin America is represented on the bird’s chest\nThe bright green background is a nod to the astonishing biodiversity in Latin America\n\n\n\nAfricaR\n\n\n\nThe AfricaR BoF button was designed with suggestions and feedback from AfricaR community leaders Shelmith Kariuki and Dennis Irorere.\nSome design elements for the AfricaR button:\n\nThe colors (orange, purple and black) are borrowed from (and inspired by) the existing Africa R Users hex design\nThe shape of the continent is in orange (and outlined by data points, with a ggplot-inspired background grid)\nThe ostrich was suggested by Shelmith and Dennis as a representative bird\n\n\n\nR-Ladies\n\n\n\nThe R-Ladies BoF button started with an idea from R-Ladies founder Gabriela de Quiroz, who suggested a gull as a nod to both R-Ladies’ beginnings in San Francisco, and to this year’s conference location!\nWith feedback from Gabriela and the R-Ladies leadership team including Laura Ación and Erin LeDell, and RStudio’s Mine Çetinkaya-Rundel, the final design features the following elements:\n\nA flock (or colony) of gulls, species chosen because it’s common to San Francisco, where R-Ladies began!\nThe gulls are riding an updraft of air together\nThe purple-to-black gradient is inspired by the R-Ladies color scheme\n\n\n\nThe whole collection\nThis year, there were 24(!) different Birds of a Feather buttons. When we put them all together, here’s the flock for the 2020 conference (with more planned for next year):\n\n\n\nWe hope that everyone can find a bird that they relate to, and we look forward to meeting many of you at future RStudio Conference BoF sessions!\nSo, just one final question: Which bird(s) are you?"
  },
  {
    "objectID": "talks_workshops.html",
    "href": "talks_workshops.html",
    "title": "Talks and Workshops",
    "section": "",
    "text": "Feb 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2022\n\n\nAllison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/urban_drainage_2022_rmarkdown_workshop/index.html",
    "href": "talks/urban_drainage_2022_rmarkdown_workshop/index.html",
    "title": "R Markdown for fun, functional and reproducible reporting",
    "section": "",
    "text": "[Slides] [Guide] [GitHub]"
  },
  {
    "objectID": "talks/sb_users_penguins_flash/index.html",
    "href": "talks/sb_users_penguins_flash/index.html",
    "title": "Tired - iris. Wired - penguins.",
    "section": "",
    "text": "[Slides]"
  },
  {
    "objectID": "talks/rstudio_global_2021/index.html",
    "href": "talks/rstudio_global_2021/index.html",
    "title": "Art lessons - A year as RStudio’s Artist-in-Residence",
    "section": "",
    "text": "[Slides] [Video]"
  },
  {
    "objectID": "talks/ucsb_gds_2020/index.html",
    "href": "talks/ucsb_gds_2020/index.html",
    "title": "Art in Data Science Education",
    "section": "",
    "text": "[Slides]"
  },
  {
    "objectID": "talks/bren_phd_workshop/index.html",
    "href": "talks/bren_phd_workshop/index.html",
    "title": "Quantitative skills and R/RStudio workshop for incoming Bren PhD students",
    "section": "",
    "text": "[Slides] [GitHub]"
  },
  {
    "objectID": "talks/ucsb_qmss_rmarkdown_workshop/index.html",
    "href": "talks/ucsb_qmss_rmarkdown_workshop/index.html",
    "title": "UCSB QMSS Workshop - Level up in R Markdown",
    "section": "",
    "text": "[Slides] [Materials] [GitHub Repo]"
  },
  {
    "objectID": "talks/dsxd_education_session_2021/index.html",
    "href": "talks/dsxd_education_session_2021/index.html",
    "title": "Creativity to learn, creativity to teach.",
    "section": "",
    "text": "[Slides]"
  },
  {
    "objectID": "talks/ucsb_ds_summit_2021/index.html",
    "href": "talks/ucsb_ds_summit_2021/index.html",
    "title": "Introducing UCSB’s New Master of Environmental Data Science Program",
    "section": "",
    "text": "[Slides]"
  },
  {
    "objectID": "talks/jsm_2021/index.html",
    "href": "talks/jsm_2021/index.html",
    "title": "Creative Contributions and Activities for Student Engagement and Learning in Data Science Education",
    "section": "",
    "text": "[Slides]"
  },
  {
    "objectID": "talks/ecodatasci_ggplot_intro/index.html",
    "href": "talks/ecodatasci_ggplot_intro/index.html",
    "title": "Intro to ggplot2 for eco-data-science",
    "section": "",
    "text": "[GitHub]"
  },
  {
    "objectID": "talks/bren_alumni_2019/index.html",
    "href": "talks/bren_alumni_2019/index.html",
    "title": "Bren alumni R-refresher workshop",
    "section": "",
    "text": "[GitHub]"
  },
  {
    "objectID": "talks/missing_explorer_2020/index.html",
    "href": "talks/missing_explorer_2020/index.html",
    "title": "How to Become a Missing Explorer",
    "section": "",
    "text": "[Slides] [Tutorial]"
  },
  {
    "objectID": "talks/rladies_shiny/index.html",
    "href": "talks/rladies_shiny/index.html",
    "title": "SB R-Ladies Intro to (spooky) Shiny",
    "section": "",
    "text": "[Slides] [GitHub]"
  },
  {
    "objectID": "talks/rladies_tunis_saudiarabia_tidyverse_intro/index.html",
    "href": "talks/rladies_tunis_saudiarabia_tidyverse_intro/index.html",
    "title": "R-Ladies Tunis & R-Ladies Dammam Workshop: A Quick Flight to the Edge of the Tidyverse",
    "section": "",
    "text": "[Website] [GitHub]"
  },
  {
    "objectID": "talks/nceas_roundtable_2022/index.html",
    "href": "talks/nceas_roundtable_2022/index.html",
    "title": "Making monsters - Where they came from, what they’ve taught me, and how you can make them, too",
    "section": "",
    "text": "[Slides]"
  },
  {
    "objectID": "talks/urban_drainage_2022/index.html",
    "href": "talks/urban_drainage_2022/index.html",
    "title": "Missing the point - missing values, misinterpretations, and missed opportunities in environmental data science",
    "section": "",
    "text": "Abstract: Getting my hands on new, raw environmental data is exciting, and so is diving into a fresh pool of data wrangling, analysis and visualization. In this talk, I consider different flavors of “missing the point,” asking “What might I be missing when I launch headfirst into a new data science project?” Motivated by past mistakes, I share three ways I’ve missed the point when starting a project due to (1) missing data, (2) misinterpretation, and (3) missed opportunity. Through personal anecdotes and public examples I highlight the risk of these “misses” in data science projects, then share tools and strategies to avoid them moving forward, so that we can all dive enthusiastically into projects a bit more responsibly.\n[Slides]"
  },
  {
    "objectID": "talks/nhs_r_2020/index.html",
    "href": "talks/nhs_r_2020/index.html",
    "title": "Illustrating R - Creative contributions for R education and engagement",
    "section": "",
    "text": "[Slides]"
  },
  {
    "objectID": "talks/qmss_seminar_teachingR_2021/index.html",
    "href": "talks/qmss_seminar_teachingR_2021/index.html",
    "title": "Tips & tools for gentle R introductions",
    "section": "",
    "text": "[Slides] [GitHub]"
  },
  {
    "objectID": "talks/r_for_excel_2020/index.html",
    "href": "talks/r_for_excel_2020/index.html",
    "title": "R for Excel Users Workshop at RStudio Conference 2020",
    "section": "",
    "text": "[eBook] [Slides] [GitHub]"
  },
  {
    "objectID": "talks/esa_2021/index.html",
    "href": "talks/esa_2021/index.html",
    "title": "Leveraging data richness of the Long Term Ecological Research Network for an R datasets package",
    "section": "",
    "text": "[Slides]"
  },
  {
    "objectID": "talks/useR_2021/index.html",
    "href": "talks/useR_2021/index.html",
    "title": "Developing a datasets based R package to teach environmental data science",
    "section": "",
    "text": "Presented virtually at the useR!2021 Conference [Slides]"
  },
  {
    "objectID": "talks/sccwrp_dataviz_2019/index.html",
    "href": "talks/sccwrp_dataviz_2019/index.html",
    "title": "SCCWRP Data Visualization Workshop",
    "section": "",
    "text": "[Slides]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Allison Horst",
    "section": "",
    "text": "I actively work to contribute to open educational resources, including the palmerpenguins R package, the lterdatasampler package, The Missing Data Book and through my data science and statistics artwork. I have worked as Artist-in-Residence at RStudio (2019 - 2020) and the National Center for Ecological Analysis and Synthesis (2018 - 2019)."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog posts",
    "section": "",
    "text": "Allison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2022\n\n\nAllison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2022\n\n\nAllison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2022\n\n\nAllison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2022\n\n\nAllison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2021\n\n\nAllison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do we do to prepare students trained in R and Python for jobs with Excel, Google Sheets and Access?\n\n\n\n\n\n\n\n\n\nMar 31, 2021\n\n\nAllison Horst and Jacqueline Nolis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nRStudio\n\n\ndplyr\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2021\n\n\nAllison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npalmerpenguins\n\n\n\n\ndata science\n\n\nR\n\n\nRStudio\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2020\n\n\nAllison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA powerful tool for remote teaching\n\n\n\n\nteaching\n\n\nR\n\n\nRStudio\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2020\n\n\nAllison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2020\n\n\nAllison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nart\n\n\nRStudio\n\n\n\n\n\n\n\n\n\n\n\nFeb 29, 2020\n\n\nAllison Horst, Curtis Kephart, Yanina Bellini\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncommunity\n\n\n\n\nThis course is for Excel users who want to add or integrate R and RStudio into their existing data analysis toolkit. It is a friendly intro to becoming a modern R user, full…\n\n\n\n\n\n\nFeb 18, 2020\n\n\nAllison Horst and Julie Lowndes\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nart\n\n\nR\n\n\nRStudio\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2019\n\n\nAllison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncommunity\n\n\nchampions\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2019\n\n\nHorst Lab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncommunity\n\n\nchampions\n\n\n\n\n\n\n\n\n\n\n\nMay 2, 2019\n\n\nAllison Horst\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2019\n\n\nAllison Horst\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2019\n\n\nAllison Horst\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Allison Horst - CV",
    "section": "",
    "text": "Currently: Data Scientist Advocate at Observable. Previously: 10+ years as a teaching faculty member at UC Santa Barbara. 10+ years experience creating and leading math, statistics, computer science and data science courses; leader in building communities of practice to support data science learners and teachers including as co-founder of the Santa Barbara Chapter of R-Ladies; active contributor to open educational resources for data science through software, tutorials, open courses and workshop materials, and original artwork for data science and statistics instruction."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Allison Horst - CV",
    "section": "Education",
    "text": "Education\nPh.D., Environmental Science and Management (2012)\n\nEmphasis: Environmental nanotoxicology\nBren School of Environmental Science & Management\nUniversity of California Center for Environmental Implications of Nanotechnology\nUniversity of California at Santa Barbara, Santa Barbara, CA 93106-5131\n\nM.S., Mechanical Engineering (2007)\n\nEmphasis: Environmental and Ocean Engineering\nDepartment of Mechanical Engineering\nUniversity of California at Santa Barbara, Santa Barbara, CA 93106-5070\n\nB.S., Chemical Engineering (2007)\n\nEmphasis: Environment, Risk and Management\nDepartment of Chemical Engineering\nUniversity of California at Santa Barbara, Santa Barbara, CA 93106-5080"
  },
  {
    "objectID": "cv.html#distinctions",
    "href": "cv.html#distinctions",
    "title": "Allison Horst - CV",
    "section": "Distinctions",
    "text": "Distinctions\nUC Santa Barbara Distinguished Teaching Award (2019)\nAwarded the 2019 campus-wide Distinguished Teaching Award at UC Santa Barbara for “excellence in teaching and inspiring innovation in teaching.”\nBren School of Environmental Science & Management Distinguished Teaching Award (2018)\nAwarded the 2018 Distinguished Teaching Award by Bren School graduate students."
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "Allison Horst - CV",
    "section": "Teaching",
    "text": "Teaching\nAssistant Teaching Professor (July 2020 - October 2022)\nBren School of Environmental Science and Management, UC Santa Barbara\nSelected courses:\n\nEDS 212: Essential Math in Environmental Data Science\nEDS 221: Scientific Programming Essentials\nEDS 411 AB: Environmental Data Science - Master’s Capstone Course\nESM 206: Statistics and Data Analysis in Environmental Science\nESM 244: Advanced Data Analysis\nESM 438: Presentation Skills for Environmental Professionals\n\nContinuing Lecturer - Data Science and Statistics (2013 - 2020)\nUC Santa Barbara"
  },
  {
    "objectID": "cv.html#selected-contributions",
    "href": "cv.html#selected-contributions",
    "title": "Allison Horst - CV",
    "section": "Selected Contributions",
    "text": "Selected Contributions\nAM Horst, AP Hill, KB Gorman (2022). Penguins data in the palmerpenguins R package: an alternative to Anderson’s Irises. The R Journal. doi: 10.32614/RJ-2022-020\nTierney, NJ and AM Horst. (2022). The Missing Book.\nAM Horst, AP Hill, KB Gorman (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/\nLowndes, JSS and Horst AM (2020). R for Excel Users Materials prepared for the 2020 RStudio Conference.\nStatistics and R artwork for learners and teachers (November 2018 - present)\nFree Creative Commons illustrations for R and statistics users, teachers and community members.github.com/allisonhorst/stats-illustrations"
  },
  {
    "objectID": "cv.html#professional-activities",
    "href": "cv.html#professional-activities",
    "title": "Allison Horst - CV",
    "section": "Professional Activities",
    "text": "Professional Activities\nEditorial Board - Journal of Open Source Education (August 2021 - present)\nOpenscapes Mentorship Program (December 2018 – June 2019)\nUnder mentor Dr. Julia Lowndes (Mozilla Open Science Fellow, NCEAS), was one of 7 selected researchers and teachers to participate in the inaugural year of the Openscapes program – a 10-week program to empower teams of environmental scientists with tools and community needed to build, use and share data science skills.\nRStudio Artist-in-Residence (October 2019 – October 2020)\nAs the first RStudio (now Posit) Artist-in-Residence, created illustrations to make learning R, and code more broadly, more accessible and welcoming to useRs. The images are used around the world in courses and presentations as friendly visual aids to help lower R learning barriers."
  },
  {
    "objectID": "cv.html#community-building",
    "href": "cv.html#community-building",
    "title": "Allison Horst - CV",
    "section": "Community Building",
    "text": "Community Building\nEnvironmental Data Science Summit - Organizing Committee Member (2021)\nOrganizing Committee member for the upcoming Environmental Data Science Summit.\nR-Ladies Santa Barbara – Co-founder, organizer, instructor & active member (2018 - present)\nR-Ladies is a global organization striving to increase diversity (and diversity more broadly) in the R-user community. Our Santa Barbara R-Ladies chapter hosts 5 – 6 events per year, including sessions aimed at: training technical skills, flash talks from R-Ladies, networking meetups, invited guest speakers, and more.\nUCSB TidyTuesday Coding Club – Founder, organizer, & participant (2019 - 2021)\nTidy Tuesday is a “weekly social data project in R” started by the online R for Data Science community that encourages participants to practice wrangling, visualizing, and sharing analyses using a semi-tidy dataset each week. At the UCSB Tidy Tuesday Coding Club, people can work together on Tidy Tuesdays or other R-related projects in-person, meet other R-users across campus, and build a more cohesive R community at UCSB in a no-stress social setting. Attendance is usually ~ 12 - 20 people from multiple departments across campus."
  }
]